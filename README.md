# ollama-api-python-client

A Docker Compose setup for running local LLMs using the Ollama container, along with a Python Alpine client designed to query the LLM via API. This setup demonstrates LLM functionalities, such as tool calling with models like Llama 3.2. It enables the model to interact with external tools, such as functions, APIs, or data services, for complex tasks like retrieving flight information, performing code execution, or accessing web data. Ideal for building interactive systems where LLMs can perform real-world tasks using external resources.


